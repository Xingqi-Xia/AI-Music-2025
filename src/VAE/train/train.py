# ä»æ•°æ®é›†éšä¾¿è¯»å–ä¸€åˆ—ï¼Œç„¶åæŠŠå®ƒimport os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset, random_split
from torch.utils.tensorboard import SummaryWriter
from torch.amp import GradScaler, autocast
from tqdm import tqdm
import numpy as np
import random
import os
# å¯¼å…¥ä½ çš„æ¨¡å‹,ç›®å‰è¿™é‡Œç”¨çš„æ˜¯GRUæ¶æ„
from models import MusicGRUVAE
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
# ==============================================================================
# è¶…å‚æ•°é…ç½® 
# ==============================================================================
CONFIG = {
    "exp_name": "vae_gru_bach_v2",     # å®éªŒåç§°
    "data_path": "classical_dataset.pt",
    
    # è®­ç»ƒå‚æ•°
    "batch_size": 3072,                
    "epochs": 100,                     # è®­ç»ƒè½®æ•°
    "learning_rate": 1e-3,
    "num_workers": 16,                 # æ•°æ®åŠ è½½çº¿ç¨‹æ•°
    
    # æ¨¡å‹å‚æ•° (å¿…é¡»ä¸ model.py ä¿æŒä¸€è‡´)
    "vocab_size": 130,                 # 0-129
    "embed_dim": 256,
    "hidden_dim": 512,
    "latent_dim": 128,
    "seq_len": 32,
    
    # KL é€€ç«ç­–ç•¥
    "kl_start_epoch": 3,               # å‰ 5 ä¸ª epoch ä¸ç®— KL Loss (è®©æ¨¡å‹å…ˆå­¦ä¼šé‡æ„)
    "kl_anneal_cycle": 40,             # ç”¨ 50 ä¸ª epoch æŠŠ beta ä» 0 å¢åŠ åˆ° 1
    "beta_max": 0.02,                   # KL æƒé‡çš„ä¸Šé™ (å¤ªé«˜ä¼šå¯¼è‡´é‡æ„å˜å·®ï¼Œå¯¹äºæˆ‘ä»¬çš„ç¦»æ•£åºåˆ—ä»»åŠ¡ï¼Œæœ€å¥½ä¸è¦é«˜äº0.1)
}

# ==============================================================================
# ğŸš€ è¾…åŠ©å‡½æ•°
# ==============================================================================

def loss_function(logits, target, mu, logvar, beta):
    """
    è®¡ç®— VAE çš„æ€» Loss
    logits: [B, Seq_Len-1, Vocab]
    target: [B, Seq_Len-1]
    mu, logvar: [B, Latent]
    beta: KL æ•£åº¦çš„æƒé‡
    """
    # 1. é‡æ„æŸå¤± (Reconstruction Loss)
    # Flatten åˆ° [B * Seq, Vocab] ä»¥è®¡ç®— CrossEntropy
    recon_loss = nn.CrossEntropyLoss()(logits.reshape(-1, logits.size(-1)), target.reshape(-1))
    
    # 2. KL æ•£åº¦ (KL Divergence)
    # å…¬å¼: -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)
    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    # å¹³å‡åˆ°æ¯ä¸ªæ ·æœ¬ (batch mean)
    kl_loss = kl_loss / logits.size(0)
    
    # æ€»æŸå¤±
    total_loss = recon_loss + beta * kl_loss
    
    return total_loss, recon_loss, kl_loss

def get_beta(epoch, config):
    """
    è®¡ç®—å½“å‰ Epoch çš„ KL æƒé‡ (Cyclic Annealing æˆ– Linear Annealing)
    è¿™é‡Œä½¿ç”¨ç®€å•çš„ Linear Annealing
    """
    if epoch < config["kl_start_epoch"]:
        return 0.0
    
    # çº¿æ€§å¢é•¿
    steps = epoch - config["kl_start_epoch"]
    beta = min(config["beta_max"], (steps / config["kl_anneal_cycle"]) * config["beta_max"])
    return beta

# ==============================================================================
# ğŸš„ è®­ç»ƒä¸»æµç¨‹
# ==============================================================================

def main():
    # 1. è®¾å¤‡é…ç½®
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ”¥ ä½¿ç”¨è®¾å¤‡: {device} (GPUæ•°é‡: {torch.cuda.device_count()})")
    
    # 2. åŠ è½½æ•°æ®
    print(f"ğŸ“¦ åŠ è½½æ•°æ®é›†: {CONFIG['data_path']} ...")
    full_data = torch.load(CONFIG['data_path'])
    
    # ç®€å•çš„ Train/Val åˆ‡åˆ† (95% è®­ç»ƒ, 5% éªŒè¯)
    train_size = int(0.95 * len(full_data))
    val_size = len(full_data) - train_size
    train_dataset, val_dataset = random_split(full_data, [train_size, val_size])
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=CONFIG["batch_size"], 
        shuffle=True, 
        num_workers=CONFIG["num_workers"],
        pin_memory=True,
        drop_last=True
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=CONFIG["batch_size"], 
        shuffle=False, 
        num_workers=CONFIG["num_workers"],
        pin_memory=True
    )
    
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ. è®­ç»ƒé›†: {len(train_dataset)}, éªŒè¯é›†: {len(val_dataset)}")

    # 3. åˆå§‹åŒ–æ¨¡å‹
    model = MusicGRUVAE(
        vocab_size=CONFIG["vocab_size"],
        embed_dim=CONFIG["embed_dim"],
        hidden_dim=CONFIG["hidden_dim"],
        latent_dim=CONFIG["latent_dim"],
        seq_len=CONFIG["seq_len"]
    ).to(device)

    # å¤šå¡å¹¶è¡Œ
    if torch.cuda.device_count() > 1:
        print("âš¡ å¯ç”¨ DataParallel å¤šå¡è®­ç»ƒ")
        model = nn.DataParallel(model)

    optimizer = optim.AdamW(model.parameters(), lr=CONFIG["learning_rate"])
    scaler = GradScaler(enabled=device.type == "cuda") # æ··åˆç²¾åº¦è®­ç»ƒï¼ˆCPU æ—¶è‡ªåŠ¨å…³é—­ï¼‰
    writer = SummaryWriter(log_dir=f"logs/{CONFIG['exp_name']}")

    # 4. è®­ç»ƒå¾ªç¯
    best_val_loss = float("inf")
    
    for epoch in range(CONFIG["epochs"]):
        model.train()
        running_recon = 0.0
        running_kl = 0.0
        
        # è·å–å½“å‰çš„ beta
        beta = get_beta(epoch, CONFIG)
        
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{CONFIG['epochs']} [Î²={beta:.4f}]")
        
        for batch in pbar:
            batch = batch.to(device) # [B, 32]
            #print("æ­£åœ¨è®­ç»ƒæ‰¹æ¬¡ï¼Œæ‰¹æ¬¡å¤§å°ï¼š", batch.size(0))
            
            # --- å…³é”®ï¼šNext Token Prediction ---
            # Encoder è¾“å…¥: å®Œæ•´çš„åºåˆ— (0 ~ 31)
            # Decoder è¾“å…¥: å®Œæ•´çš„åºåˆ— (æ¨¡å‹å†…éƒ¨å¤„ç† Teacher Forcing)
            # Target (æ ‡ç­¾): åº”è¯¥æ˜¯è¾“å…¥å‘å·¦ç§»åŠ¨ä¸€ä½
            #   Input: [A, B, C, D]
            #   Logits å¯¹åº”: [pred_B, pred_C, pred_D, pred_E]
            #   æ‰€ä»¥æˆ‘ä»¬éœ€è¦æ‹¿ logits[:, :-1] å’Œ batch[:, 1:] æ¯”è¾ƒ
            #   å³ï¼šçœ‹åˆ° A é¢„æµ‹ Bï¼Œçœ‹åˆ° B é¢„æµ‹ C...
            
            optimizer.zero_grad()
            
            # ä½¿ç”¨ torch.amp.autocast å…¼å®¹æ–°ç‰ˆæœ¬ï¼›CPU æ—¶è‡ªåŠ¨å…³é—­æ··åˆç²¾åº¦
            with autocast(device_type=device.type, dtype=torch.bfloat16 if device.type == "cuda" else None):
                logits, mu, logvar = model(batch) 
                
                # å¯¹é½ Logits å’Œ Targets
                # logits: [B, 32, Vocab] -> å–å‰ 31 ä¸ªé¢„æµ‹
                logits_pred = logits[:, :-1, :]
                # targets: [B, 32] -> å–å 31 ä¸ªçœŸå®å€¼
                targets = batch[:, 1:]
                
                loss, recon, kl = loss_function(logits_pred, targets, mu, logvar, beta)
            #print("è®¡ç®—æŸå¤±å®Œæˆï¼Œå¼€å§‹åå‘ä¼ æ’­")
            # åå‘ä¼ æ’­
            scaler.scale(loss).backward()
            
            # æ¢¯åº¦è£å‰ª (é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸)
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            scaler.step(optimizer)
            scaler.update()
            #print("ä¼˜åŒ–å™¨æ­¥éª¤å®Œæˆ")
            # è®°å½•
            running_recon += recon.item()
            running_kl += kl.item()
            
            pbar.set_postfix({"Recon": f"{recon.item():.4f}", "KL": f"{kl.item():.4f}"})

        # --- Epoch ç»“æŸè®°å½• ---
        avg_recon = running_recon / len(train_loader)
        avg_kl = running_kl / len(train_loader)
        
        writer.add_scalar("Train/Recon_Loss", avg_recon, epoch)
        writer.add_scalar("Train/KL_Loss", avg_kl, epoch)
        writer.add_scalar("Train/Beta", beta, epoch)

        # --- éªŒè¯å¾ªç¯ ---
        model.eval()
        val_recon = 0.0
        with torch.no_grad():
            for batch in val_loader:
                batch = batch.to(device)
                logits, mu, logvar = model(batch)
                
                logits_pred = logits[:, :-1, :]
                targets = batch[:, 1:]
                
                _, recon, _ = loss_function(logits_pred, targets, mu, logvar, beta=1.0) # éªŒè¯æ—¶é€šå¸¸çœ‹çº¯ç²¹çš„æŒ‡æ ‡
                val_recon += recon.item()
        
        avg_val_loss = val_recon / len(val_loader)
        writer.add_scalar("Val/Recon_Loss", avg_val_loss, epoch)
        
        print(f"ğŸ“Š Validation Loss: {avg_val_loss:.4f}")

        # --- ä¿å­˜æ¨¡å‹ ---
        # ä¿å­˜æœ€æ–°
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
        }
        torch.save(checkpoint, f"checkpoints/{CONFIG['exp_name']}_latest.pth")
        
        # ä¿å­˜æœ€ä½³
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(checkpoint, f"checkpoints/{CONFIG['exp_name']}_best.pth")
            print("ğŸŒŸ New Best Model Saved!")

    writer.close()
    print("ğŸ è®­ç»ƒå®Œæˆï¼")

if __name__ == "__main__":
    # åˆ›å»ºæ–‡ä»¶å¤¹
    os.makedirs("checkpoints", exist_ok=True)
    os.makedirs("logs", exist_ok=True)
    main()